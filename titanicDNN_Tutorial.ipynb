{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30715,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sonnylowe/titanic-simple-deep-learning-tutorial?scriptVersionId=181117638\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# **Titanic - Simple Deep Learning Tutorial**\n\nWelcome! This simple tutorial walks you through simple data processing and using Kera's Sequential framework to generate a deep neural network to model our data. You will learn basic concepts and develop a baseline model that is impressive in accuracy. Nevertheless, you are encouraged to experiment with the data and the model to generate a more accurate prediction. \n\nThis titanic dataset is relatively simple and small, meaning deep learning might actually be overkill and more inaccurate than simpler decision trees and linear regression. Despite this, the skills you will learn on this simpler problem can be applied to more complex datasets in your problem solving future.\n\nIf you learned something, please upvote to support these types of tutorials!\n\nBy Sonny Lowe","metadata":{}},{"cell_type":"markdown","source":"# File Processing:\nReading the input file csv.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-06-02T17:12:32.059267Z","iopub.execute_input":"2024-06-02T17:12:32.059666Z","iopub.status.idle":"2024-06-02T17:12:32.067165Z","shell.execute_reply.started":"2024-06-02T17:12:32.059637Z","shell.execute_reply":"2024-06-02T17:12:32.06571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Processing:","metadata":{}},{"cell_type":"markdown","source":"Our data processing is split in two steps: one predefined custom function for the more complex data columns (ie name, ticket, etc), and then a processing keras pipeline for general numerical and categorical data. \n\nFirst, our custom function, called preprocess, isolates the prefix in a name (miss, mr, mrs) and places it into its own column. It also splits the ticket into the ticket number and its class (item).\n\nFinally, we will split the data into two sections, **training** and **validation**. The size of the training versus validation sections can be specified through the *train_size* property.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\n\nfile_path = '../input/titanic/train.csv'\ndata = pd.read_csv(file_path)\ndata.head()\n\nX = data.copy()\ny = X.Survived\n\ndef preprocess(df):\n    df = df.copy()\n    \n    def prefix(x):\n        name = x.split(\",\")\n        return (name[1].split(\" \")[1])[:-1]\n    \n    def normalize_name(x):\n        name = \" \".join([v.strip(\",()[].\\\"'\") for v in x.split(\" \")])\n        return name.split(\" \")[0] + \" \" + \" \".join(name.split(\" \")[2:])\n    \n    def ticket_number(x):\n        num = x.split(\" \")[-1]\n        if(num.isnumeric()):\n            return num\n        return 0\n        \n    def ticket_item(x):\n        items = x.split(\" \")\n        if len(items) == 1:\n            return \"NONE\"\n        return \"_\".join(items[0:-1])\n    \n    df[\"Prefix\"] = df[\"Name\"].apply(prefix)\n    df[\"Name\"] = df[\"Name\"].apply(normalize_name)\n    df[\"Ticket_number\"] = df[\"Ticket\"].apply(ticket_number)\n    df[\"Ticket_item\"] = df[\"Ticket\"].apply(ticket_item)\n        \n    return df\n\nX = preprocess(X)\n\nprefix_possibilities = X['Prefix'].unique()\nprint(prefix_possibilities)\n\nX_train, X_valid, y_train, y_valid = \\\n    train_test_split(X, y, stratify=y, train_size=0.75)\n\nX.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T17:12:34.05605Z","iopub.execute_input":"2024-06-02T17:12:34.056462Z","iopub.status.idle":"2024-06-02T17:12:34.102949Z","shell.execute_reply.started":"2024-06-02T17:12:34.056427Z","shell.execute_reply":"2024-06-02T17:12:34.101585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The features are split into two cartegories for pipeline processing - **Numberical** and **Categorical**.\n- A data **pipeline** takes in raw data and processes it into the desired format using a fixed framework\n- This framework first **imputes** the numerical data by substituting missing data with a different value. Then it **scales** it to unit variance.\n- For categorical data, it uses **OneHotEncoding** to transform categorical data to multiple columns of each possible unique entry, with a 0 if it exists and a 1 if it does not. See the diagram below:\n\n![1_ggtP4a5YaRx6l09KQaYOnw.png](attachment:8b055ab6-cb25-4640-8e3a-11a9d98be05b.png)","metadata":{},"attachments":{"8b055ab6-cb25-4640-8e3a-11a9d98be05b.png":{"image/png":"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"}}},{"cell_type":"code","source":"features_num = [\n    'Pclass', 'Age', 'SibSp', 'Parch', 'Ticket_number'\n]\n\nfeatures_cat = [\n    'Sex', 'Embarked', 'Prefix'\n]\n\n# these are features that we want to keep but need not process. In this case, it is empty but feel free to add some\nfeatures_other = [\n]\n\ntransformer_num = make_pipeline(\n    SimpleImputer(),\n    StandardScaler()\n)\n\ntransformer_cat = make_pipeline(\n    SimpleImputer(strategy=\"constant\", fill_value=\"NA\"),\n    OneHotEncoder(handle_unknown='ignore', sparse_output=False),\n)\n\npreprocessor = make_column_transformer(\n    (transformer_num, features_num),\n    (transformer_cat, features_cat),\n    ('passthrough', features_other)\n)\n\nprocessed_X_train = preprocessor.fit_transform(X_train)\nprocessed_X_valid = preprocessor.transform(X_valid)\n\ndf = pd.DataFrame(processed_X_train)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T17:12:53.115792Z","iopub.execute_input":"2024-06-02T17:12:53.116954Z","iopub.status.idle":"2024-06-02T17:12:53.170778Z","shell.execute_reply.started":"2024-06-02T17:12:53.11689Z","shell.execute_reply":"2024-06-02T17:12:53.169443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model\nUsing Keras, we will create the structure of our neural network. Determining the amount of layers, dropout layers, and batch normalizations is a complex process, and you are encouraged to experiment with the structure. The given structure is by no means an optimal one.\n\nThis model uses two basic types of layers\n- **Dense layers:** these are the most basic layers of a neural network that are composed of nodes (neurons) that take inputs and linearly alters it to an output (very simplified explanation). Then, they each will have an activation function that essentially places a kink in the linearly path to allow the model to stray from pure line.\n- **Dropout Layers:** A dropout layer is a layer in a neural network that randomly sets some input units to zero during training to help prevent overfitting. This is done with a defined probability, or rate, which is specified through the *rate* property.\n\nYou may also experiment with batch normalization layers:\n- **Batch Normalization:** Batch normalization is a technique that normalizes the inputs of each layer in a deep neural network to improve training speed and stability.","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.Dense(256, activation='relu'),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(rate=0.2),\n    layers.Dense(256, activation='relu'),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(rate=0.2),\n    layers.Dense(256, activation='relu'),\n    layers.Dense(1, activation='sigmoid')\n])","metadata":{"execution":{"iopub.status.busy":"2024-06-02T17:12:57.754765Z","iopub.execute_input":"2024-06-02T17:12:57.755822Z","iopub.status.idle":"2024-06-02T17:12:57.778053Z","shell.execute_reply.started":"2024-06-02T17:12:57.755776Z","shell.execute_reply":"2024-06-02T17:12:57.776531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This model's goal is a binary output - 0 for death and 1 for survival. Therefore, when **compiling** this model, we can specify our loss function and metric to be of binary nature. A loss function is the function that the model will attempt to minimize, which can be thought of as a measure of departure from accuracy. Obviously, we will attempt to minimize this departure.\n\nIn addition, to prevent overfitting, we have an **early_stopping** function. Essentially, we can specify how many epochs to wait (*patience*) without a minimum improvement (*min_delta*) in order to prematurely stop the compilation. You can play around with these properties too.\n\nFinally, we will track the **history** of our model and how many *epochs* (versions) to run it for. This number is unnecessarily large, as we are relying on our early stopping function to terminate the running. You can also change the batch_sizes, which is how much data it looks at for each iteration in an epoch. (Every epoch is made of many iterations towards the best possible accuracy)","metadata":{}},{"cell_type":"code","source":"model.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy'],\n)\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=20,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\n\nhistory = model.fit(\n    processed_X_train, y_train,\n    validation_data=(processed_X_valid, y_valid),\n    batch_size=512,\n    epochs=100,\n    callbacks=[early_stopping],\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T17:13:06.11688Z","iopub.execute_input":"2024-06-02T17:13:06.117284Z","iopub.status.idle":"2024-06-02T17:13:11.049143Z","shell.execute_reply.started":"2024-06-02T17:13:06.117252Z","shell.execute_reply":"2024-06-02T17:13:11.047977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we can plot this history.","metadata":{}},{"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title=\"Accuracy\")\n\nprint((\"Best Validation Loss: {:0.4f}\" +\\\n      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n      .format(history_df['val_loss'].min(), \n              history_df['val_binary_accuracy'].max()))","metadata":{"execution":{"iopub.status.busy":"2024-06-02T17:13:13.436454Z","iopub.execute_input":"2024-06-02T17:13:13.436876Z","iopub.status.idle":"2024-06-02T17:13:14.03743Z","shell.execute_reply.started":"2024-06-02T17:13:13.436841Z","shell.execute_reply":"2024-06-02T17:13:14.036417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting\n\nUsing our established data processing pipeline, we can process the given testing data. Then, we will use the model with the best accuracy across epochs, as our early stopping function tracks, to determine predictions.\n\nThe final step is writing the predictions into an output file to submit. It is now ur job to get a better score than we have here! Good Luck!","metadata":{}},{"cell_type":"code","source":"subm_path = '../input/titanic/test.csv'\n\ndata = pd.read_csv(subm_path)\ntest_data = data.copy()\n\ntest_data = preprocess(test_data)\ntest_data = preprocessor.transform(test_data)\n\nfinal_preds = np.round(np.clip(model.predict(test_data), 0, 1))\n\noutput = pd.DataFrame({\n    'PassengerId': data.PassengerId,\n    'Survived': final_preds[:, 0].astype(int)\n})\n\n#uncomment the below lines to write the output into your submission file.\noutput.to_csv('submission.csv', index=False)\noutput = pd.read_csv('submission.csv')\noutput.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T17:13:22.781082Z","iopub.execute_input":"2024-06-02T17:13:22.781481Z","iopub.status.idle":"2024-06-02T17:13:23.037397Z","shell.execute_reply.started":"2024-06-02T17:13:22.78145Z","shell.execute_reply":"2024-06-02T17:13:23.036292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Tuning\nIn this section, we will experiment with different parameters to find the best combination for this dataset. There are certain tools we can use to make it easier for us (**keras Tuner**, grid search, random search, etc).\n\nIn this example, we will use the Keras Tuner to tune many different parameters including:\n- **number of dense layers**\n- **presence of dropout layers**\n- **dense unit count**\n- **dropout rate**\n- **activation function**\n- **optimizer**\n\nGiven the relatively small dataset, the parameters might not make a huge difference. However, this is still a very useful skill to learn when applying it to larger more complex datasets.","metadata":{}},{"cell_type":"code","source":"import keras_tuner\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\ndef build_model(hp):\n    model = keras.Sequential()\n    \n    for i in range(hp.Int(\"num_layers\", 1, 2)):\n        model.add(\n            layers.Dense(\n                # Tune number of units separately.\n                units=hp.Int(f\"units_{i}\", min_value=32, max_value=512, step=32),\n                activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n            )\n        )\n    if hp.Boolean(\"dropout\"):\n        model.add(layers.Dropout(0.2))\n        \n    for i in range(hp.Int(\"num_layers\", 1, 2)):\n        model.add(\n            layers.Dense(\n                # Tune number of units separately.\n                units=hp.Int(f\"units_{i}\", min_value=32, max_value=512, step=32),\n                activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n            )\n        )\n    if hp.Boolean(\"dropout\"):\n        model.add(layers.Dropout(0.2))\n    \n    model.add(layers.Dense(1, activation=\"sigmoid\"))\n    \n    model.compile(\n        optimizer=hp.Choice(\"optimizer\", ['adam', 'sgd', 'rmsprop']),\n        loss=\"binary_crossentropy\",\n        metrics=[\"binary_accuracy\"],\n    )\n    return model\n\n# testing if it builds successfully (expected: <Sequential name=sequential_#, built=False>)\nbuild_model(keras_tuner.HyperParameters())","metadata":{"execution":{"iopub.status.busy":"2024-06-02T17:04:46.785683Z","iopub.execute_input":"2024-06-02T17:04:46.786055Z","iopub.status.idle":"2024-06-02T17:04:46.810945Z","shell.execute_reply.started":"2024-06-02T17:04:46.786026Z","shell.execute_reply":"2024-06-02T17:04:46.809651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we will actually begin the tuner using a random search and some specified parameters. The important ones to take note of here are the max_trials and executions per trial. Similar to a lab experiement, each trial has different parameters. However, due to the variance inherent within the models, we can choose to execute each trial multiple times. Here is a more comprehensive tutorial on hyperparameter tuning: [https://keras.io/guides/keras_tuner/getting_started/](http://)","metadata":{}},{"cell_type":"code","source":"tuner = keras_tuner.RandomSearch(\n    hypermodel=build_model,\n    objective=\"binary_accuracy\",\n    max_trials=15,\n    executions_per_trial=3,\n    overwrite=True\n)\n\n# confirming the parameters of our search\ntuner.search_space_summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T17:04:46.812314Z","iopub.execute_input":"2024-06-02T17:04:46.812733Z","iopub.status.idle":"2024-06-02T17:04:46.8371Z","shell.execute_reply.started":"2024-06-02T17:04:46.812694Z","shell.execute_reply":"2024-06-02T17:04:46.835868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Actually starting the search! This will take a while depending on how many trials you put. After it has searched, we can ask it to give us a summary of the best models and their respective parameters.","metadata":{}},{"cell_type":"code","source":"# uncomment this code to begin the search\n\n# early_stopping = keras.callbacks.EarlyStopping(\n#     patience=5,\n#     min_delta=0.001,\n#     restore_best_weights=True,\n# )\n\n# tuner.search(processed_X_train, y_train, epochs=100, batch_size=512, validation_data=(processed_X_valid, y_valid), callbacks=[early_stopping], verbose=0)\n# tuner.results_summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T17:04:46.83843Z","iopub.execute_input":"2024-06-02T17:04:46.838764Z","iopub.status.idle":"2024-06-02T17:04:46.845466Z","shell.execute_reply.started":"2024-06-02T17:04:46.838735Z","shell.execute_reply":"2024-06-02T17:04:46.843979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we can isolate the best model and ask for its parameters.","metadata":{}},{"cell_type":"code","source":"# best_model = tuner.get_best_models(num_models=1)[0]\n# best_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T17:04:46.84952Z","iopub.execute_input":"2024-06-02T17:04:46.850057Z","iopub.status.idle":"2024-06-02T17:04:46.854876Z","shell.execute_reply.started":"2024-06-02T17:04:46.850017Z","shell.execute_reply":"2024-06-02T17:04:46.853884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we will use these found best parameters to retrain the model on the entire dataset to be used for submission.","metadata":{}},{"cell_type":"code","source":"# Uncomment below to Get the top hyperparameter\n\n# best_hps = tuner.get_best_hyperparameters()[0]\n# final_model = build_model(best_hps)\n\n# x_all = np.concatenate((processed_X_train, processed_X_valid))\n# y_all = np.concatenate((y_train, y_valid))\n\n# final_model.fit(x_all, y_all, epochs=30)\n\n# subm_path = '../input/titanic/test.csv'\n\n# data = pd.read_csv(subm_path)\n# test_data = data.copy()\n\n# test_data = preprocess(test_data)\n# test_data = preprocessor.transform(test_data)\n\n# final_preds = np.round(np.clip(final_model.predict(test_data), 0, 1))\n\n# output = pd.DataFrame({\n#     'PassengerId': data.PassengerId,\n#     'Survived': final_preds[:, 0].astype(int)\n# })\n\n#uncomment the below lines to write the output into your submission file.\n#output.to_csv('submission.csv', index=False)\n\n#output = pd.read_csv('submission.csv')\n#output.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T17:04:46.856058Z","iopub.execute_input":"2024-06-02T17:04:46.856365Z","iopub.status.idle":"2024-06-02T17:04:46.866299Z","shell.execute_reply.started":"2024-06-02T17:04:46.856321Z","shell.execute_reply":"2024-06-02T17:04:46.865209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bonus Section\nTaking the Majority Guess of Many Models without using hyperparameterization.","metadata":{}},{"cell_type":"code","source":"def train_model(X, y, test_data):\n\n    X_train, X_valid, p_y_train, p_y_valid = \\\n        train_test_split(X, y, stratify=y, train_size=0.75)\n    \n    p_X_train = preprocessor.fit_transform(X_train)\n    p_X_valid = preprocessor.transform(X_valid)\n    pred_data = test_data.copy()\n    pred_data = preprocessor.transform(pred_data)\n    \n    new_model = keras.Sequential([\n        layers.Dense(256, activation='relu'),\n        layers.Dense(256, activation='relu'),\n        layers.Dropout(rate=0.2),\n        layers.Dense(256, activation='relu'),\n        layers.Dense(256, activation='relu'),\n        layers.Dropout(rate=0.2),\n        layers.Dense(256, activation='relu'),\n        layers.Dense(1, activation='sigmoid'),\n    ])\n    new_model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['binary_accuracy'],\n    )\n    early_stopping = keras.callbacks.EarlyStopping(\n        patience=10,\n        min_delta=0.001,\n        restore_best_weights=True,\n    )\n    new_history = new_model.fit(\n        p_X_train, p_y_train,\n        validation_data=(p_X_valid, p_y_valid),\n        batch_size=512,\n        epochs=100,\n        callbacks=[early_stopping],\n        verbose=0\n    )\n\n    preds = np.round(np.clip(new_model.predict(pred_data), 0, 1))\n    histories.append(new_history)\n    return preds\n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-02T17:04:46.867538Z","iopub.execute_input":"2024-06-02T17:04:46.867957Z","iopub.status.idle":"2024-06-02T17:04:46.879439Z","shell.execute_reply.started":"2024-06-02T17:04:46.867871Z","shell.execute_reply":"2024-06-02T17:04:46.878287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"histories = []\n\nfile_path = '../input/titanic/train.csv'\ndata = pd.read_csv(file_path)\n\nX = data.copy()\ny = X.Survived\nX = preprocess(X)\n\nsubm_path = '../input/titanic/test.csv'\ntest_data = pd.read_csv(subm_path)\ntest_data = preprocess(test_data)\n\n# arrays to track the number of predictions for a passenger to have died or survived\n# the final prediction will be determined by which prediction is more frequent with tiebreaks for dying\ndead = []\nsurvived = []\n\n# change the range for how many models you want to create - the more models the more time\n# uncomment the below to run the compilation\n# for x in range(15):\n#    preds = train_model(X, y, test_data)\n       \n#    for i in range(0, len(preds)):\n#        if(x == 0):\n#            if (preds[i] == 0):\n#                dead.append(1)\n#                survived.append(0)\n#            else:\n#                dead.append(0)\n#                survived.append(1)\n#        else:\n#             if (preds[i] == 0):\n#                 dead[i] += 1\n#             else:\n#                 survived[i] += 1\n\n# print(\"Binary Accuracies:\")\n# for x in range(0, len(histories)):\n#     history_df = pd.DataFrame(histories[x].history)\n#     print(history_df['val_binary_accuracy'].max())","metadata":{"execution":{"iopub.status.busy":"2024-06-02T17:04:46.880795Z","iopub.execute_input":"2024-06-02T17:04:46.881131Z","iopub.status.idle":"2024-06-02T17:04:46.916587Z","shell.execute_reply.started":"2024-06-02T17:04:46.881103Z","shell.execute_reply":"2024-06-02T17:04:46.915483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_preds = []\n\n# for i in range(0, len(dead)):\n#     if(dead[i] >= survived[i]):\n#         final_preds.append(0)\n#     else:\n#         final_preds.append(1)\n\n# output = pd.DataFrame({\n#     'PassengerId': test_data.PassengerId,\n#     'Survived': final_preds\n# })\n\n# output.to_csv('submission.csv', index=False)\n# output = pd.read_csv('submission.csv')\n# output.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T17:04:46.91806Z","iopub.execute_input":"2024-06-02T17:04:46.919062Z","iopub.status.idle":"2024-06-02T17:04:46.924238Z","shell.execute_reply.started":"2024-06-02T17:04:46.91902Z","shell.execute_reply":"2024-06-02T17:04:46.922964Z"},"trusted":true},"execution_count":null,"outputs":[]}]}